{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKDyph7C5mei"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torch-scatter\n",
        "!pip install gudhi\n",
        "!pip install torchdiffeq\n",
        "!pip install scikit-optimize\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main model"
      ],
      "metadata": {
        "id": "AJJvDO3d6Ash"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "\n",
        "import gudhi as gd\n",
        "import numpy as np\n",
        "\n",
        "from torchdiffeq import odeint\n",
        "\n",
        "import optuna\n",
        "import random\n",
        "\n",
        "# DeepSets-векторизация для персистентных диаграмм\n",
        "class DeepSetPH(nn.Module):\n",
        "    def __init__(self, phi_dim=64, out_dim=32):\n",
        "        super().__init__()\n",
        "        self.phi = nn.Sequential(\n",
        "            nn.Linear(2, phi_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(phi_dim, phi_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.rho = nn.Sequential(\n",
        "            nn.Linear(phi_dim, out_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, diagrams):\n",
        "        batch_embeds = []\n",
        "        for dgm in diagrams:\n",
        "            if len(dgm) == 0:\n",
        "                emb = torch.zeros(self.rho[0].out_features, device=self.rho[0].weight.device)\n",
        "            else:\n",
        "                dgm_tensor = torch.tensor(dgm, dtype=torch.float32, device=self.rho[0].weight.device)\n",
        "                phi_out = self.phi(dgm_tensor)\n",
        "                pooled = phi_out.sum(dim=0)\n",
        "                emb = self.rho(pooled)\n",
        "            batch_embeds.append(emb.unsqueeze(0))\n",
        "        return torch.cat(batch_embeds, dim=0)\n",
        "\n",
        "# Функция для вычисления локальных Персистентных диаграмм (H_0 и H_1)\n",
        "def compute_local_ph_diagrams(x, edge_index, k=1, maxdim=1, hom_dims=[0, 1]):\n",
        "    num_nodes = x.size(0)\n",
        "    x_np = x.detach().cpu().numpy()\n",
        "    edge_index = edge_index.cpu()\n",
        "    diagrams = []\n",
        "    for v in range(num_nodes):\n",
        "        subset, sub_edge_index, _, _ = k_hop_subgraph(v, k, edge_index, relabel_nodes=True)\n",
        "        pts = x_np[subset.numpy()]\n",
        "        if pts.shape[0] < 2:\n",
        "            dgm = np.array([[0.0, 0.0]])\n",
        "        else:\n",
        "            try:\n",
        "                rips = gd.RipsComplex(points=pts, max_edge_length=10.0)\n",
        "                st = rips.create_simplex_tree(max_dimension=maxdim)\n",
        "                st.persistence()\n",
        "                dgm = []\n",
        "                for h in hom_dims:\n",
        "                    dgm_h = st.persistence_intervals_in_dimension(h)\n",
        "                    dgm += dgm_h\n",
        "                dgm = np.array([p for p in dgm if np.isfinite(p[1])])\n",
        "                if len(dgm) == 0:\n",
        "                    dgm = np.array([[0.0, 0.0]])\n",
        "            except Exception as e:\n",
        "                dgm = np.array([[0.0, 0.0]])\n",
        "        diagrams.append(dgm)\n",
        "    return diagrams\n",
        "\n",
        "# Стандартный слой TopNets\n",
        "class TopNetsLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, phi_dim=64, ph_out_dim=32):\n",
        "        super().__init__()\n",
        "        self.msg_linear = nn.Linear(in_channels, out_channels)\n",
        "        self.ph_encoder = DeepSetPH(phi_dim=phi_dim, out_dim=ph_out_dim)\n",
        "        self.update_linear = nn.Linear(out_channels + ph_out_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "        msg = self.msg_linear(x)\n",
        "        aggr = torch.zeros_like(msg)\n",
        "        aggr.index_add_(0, row, msg[col])\n",
        "        diagrams = compute_local_ph_diagrams(x, edge_index, k=1, maxdim=1, hom_dims=[0, 1])\n",
        "        ph_embed = self.ph_encoder(diagrams)\n",
        "        combined = torch.cat([aggr, ph_embed], dim=-1)\n",
        "        return F.relu(self.update_linear(combined))\n",
        "\n",
        "# Эквивариантный слой TopNets\n",
        "class EquivariantTopNetsLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, phi_dim=64, ph_out_dim=32):\n",
        "        super().__init__()\n",
        "        self.msg_linear = nn.Linear(in_channels, out_channels)\n",
        "        self.ph_encoder = DeepSetPH(phi_dim=phi_dim, out_dim=ph_out_dim)\n",
        "        self.update_linear = nn.Linear(out_channels + ph_out_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, x_coord, edge_index):\n",
        "        row, col = edge_index\n",
        "        msg = self.msg_linear(x)\n",
        "        aggr = torch.zeros_like(msg)\n",
        "        aggr.index_add_(0, row, msg[col])\n",
        "        diagrams = compute_local_ph_diagrams(x_coord, edge_index, k=1, maxdim=1, hom_dims=[0, 1])\n",
        "        ph_embed = self.ph_encoder(diagrams)\n",
        "        combined = torch.cat([aggr, ph_embed], dim=-1)\n",
        "        return F.relu(self.update_linear(combined))\n",
        "\n",
        "# Continuous TopNets (Neural ODE)\n",
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feature_dim, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        return self.f(x)\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "    def __init__(self, odefunc, t0=0.0, t1=1.0, tol=1e-3):\n",
        "        super().__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([t0, t1]).float()\n",
        "        self.tol = tol\n",
        "\n",
        "    def forward(self, x):\n",
        "        integration_time = self.integration_time.type_as(x)\n",
        "        out = odeint(self.odefunc, x, integration_time, rtol=self.tol, atol=self.tol)\n",
        "        return out[1]\n",
        "\n",
        "# модель TopNets с режимами 'standard', 'equivariant' и 'continuous'\n",
        "class UnifiedTopNets(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_classes, mode='standard', phi_dim=64, ph_out_dim=32):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.readout = global_mean_pool\n",
        "\n",
        "        if mode == 'standard':\n",
        "            self.layers = nn.ModuleList()\n",
        "            self.layers.append(TopNetsLayer(in_channels, hidden_channels, phi_dim, ph_out_dim))\n",
        "            for _ in range(num_layers - 1):\n",
        "                self.layers.append(TopNetsLayer(hidden_channels, hidden_channels, phi_dim, ph_out_dim))\n",
        "            self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "        elif mode == 'equivariant':\n",
        "            self.layers = nn.ModuleList()\n",
        "            self.layers.append(EquivariantTopNetsLayer(in_channels, hidden_channels, phi_dim, ph_out_dim))\n",
        "            for _ in range(num_layers - 1):\n",
        "                self.layers.append(EquivariantTopNetsLayer(hidden_channels, hidden_channels, phi_dim, ph_out_dim))\n",
        "            self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "        elif mode == 'continuous':\n",
        "            self.encoder = nn.Linear(in_channels, hidden_channels)\n",
        "            self.ph_encoder = DeepSetPH(phi_dim=phi_dim, out_dim=ph_out_dim)\n",
        "            self.odefunc = ODEFunc(hidden_channels + ph_out_dim)\n",
        "            self.odeblock = ODEBlock(self.odefunc)\n",
        "            self.classifier = nn.Linear(hidden_channels + ph_out_dim, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        if self.mode == 'standard':\n",
        "            for layer in self.layers:\n",
        "                x = layer(x, edge_index)\n",
        "            x = self.readout(x, batch)\n",
        "            return self.classifier(x)\n",
        "\n",
        "        elif self.mode == 'equivariant':\n",
        "            if hasattr(data, \"pos\") and data.pos is not None:\n",
        "                x_coord = data.pos\n",
        "            else:\n",
        "                x_coord = x\n",
        "            for layer in self.layers:\n",
        "                x = layer(x, x_coord, edge_index)\n",
        "            x = self.readout(x, batch)\n",
        "            return self.classifier(x)\n",
        "\n",
        "        elif self.mode == 'continuous':\n",
        "            x = self.encoder(x)\n",
        "            diagrams = compute_local_ph_diagrams(x, edge_index, k=1, maxdim=1, hom_dims=[0, 1])\n",
        "            ph_embed = self.ph_encoder(diagrams)\n",
        "            x = torch.cat([x, ph_embed], dim=-1)\n",
        "            x = self.odeblock(x)\n",
        "            x = self.readout(x, batch)\n",
        "            return self.classifier(x)\n",
        "\n",
        "# Функции тренировки и тестирования\n",
        "def train_epoch(model, optimizer, criterion, loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# байес\n",
        "def objective(trial, dataset_name, device):\n",
        "    mode = trial.suggest_categorical(\"mode\", [\"standard\", \"equivariant\", \"continuous\"])\n",
        "    hidden_channels = trial.suggest_int(\"hidden_channels\", 32, 128, step=16)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    phi_dim = trial.suggest_int(\"phi_dim\", 32, 128, step=16)\n",
        "    ph_out_dim = trial.suggest_int(\"ph_out_dim\", 16, 64, step=8)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-2)\n",
        "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    dataset = TUDataset(root='/tmp/' + dataset_name, name=dataset_name)\n",
        "    dataset = dataset.shuffle()\n",
        "    split_idx = int(0.8 * len(dataset))\n",
        "    train_dataset = dataset[:split_idx]\n",
        "    test_dataset = dataset[split_idx:]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = UnifiedTopNets(\n",
        "        in_channels=dataset.num_node_features,\n",
        "        hidden_channels=hidden_channels,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=dataset.num_classes,\n",
        "        mode=mode,\n",
        "        phi_dim=phi_dim,\n",
        "        ph_out_dim=ph_out_dim\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, optimizer, criterion, train_loader, device)\n",
        "        acc = test_epoch(model, test_loader, device)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        trial.report(acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return best_acc\n",
        "\n",
        "#запуск экспериментов на нескольких датасетах с подбором гиперпараметров\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset_names = [\"MUTAG\", \"PROTEINS\", \"IMDB-BINARY\", \"NCI1\"]\n",
        "    results = {}\n",
        "    for ds in dataset_names:\n",
        "        print(f\"\\n=== Запуск оптимизации для датасета: {ds} ===\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(lambda trial: objective(trial, ds, device), n_trials=20)\n",
        "        print(f\"Лучшее значение для {ds}: {study.best_value:.4f}\")\n",
        "        print(f\"Лучшие гиперпараметры для {ds}: {study.best_params}\")\n",
        "        results[ds] = {\"best_value\": study.best_value, \"best_params\": study.best_params}\n",
        "\n",
        "    print(\"\\n=== Итоговые результаты по датасетам ===\")\n",
        "    for ds in dataset_names:\n",
        "        print(f\"{ds}: Acc = {results[ds]['best_value']:.4f}, Params = {results[ds]['best_params']}\")"
      ],
      "metadata": {
        "id": "nnCSLUBz6AYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My MLP model"
      ],
      "metadata": {
        "id": "X3HH9nOM51vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# MLP модель\n",
        "class MLPGraph(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(in_channels, hidden_channels))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "            layers.append(nn.ReLU())\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, batch = data.x, data.batch\n",
        "        if x is None:\n",
        "            num_nodes = data.num_nodes\n",
        "            x = torch.ones((num_nodes, 1), device=data.edge_index.device)\n",
        "        x = self.mlp(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Тренировка и тест\n",
        "def train_epoch(model, optimizer, criterion, loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# байес\n",
        "def objective_mlp(trial, dataset_name, device):\n",
        "    hidden_channels = trial.suggest_int(\"hidden_channels\", 32, 128, step=16)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-2)\n",
        "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    dataset = TUDataset(root='/tmp/' + dataset_name, name=dataset_name)\n",
        "    dataset = dataset.shuffle()\n",
        "    split_idx = int(0.8 * len(dataset))\n",
        "    train_dataset = dataset[:split_idx]\n",
        "    test_dataset = dataset[split_idx:]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    in_channels = dataset.num_node_features\n",
        "    if in_channels == 0:\n",
        "        in_channels = 1\n",
        "\n",
        "    model = MLPGraph(\n",
        "        in_channels=in_channels,\n",
        "        hidden_channels=hidden_channels,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=dataset.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        train_epoch(model, optimizer, criterion, train_loader, device)\n",
        "        acc = test_epoch(model, test_loader, device)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        trial.report(acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return best_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset_names = [\"MUTAG\", \"PROTEINS\", \"IMDB-BINARY\", \"NCI1\"]\n",
        "    results = {}\n",
        "    for ds in dataset_names:\n",
        "        print(f\"\\n=== Оптимизация для датасета: {ds} (MLP) ===\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(lambda trial: objective_mlp(trial, ds, device), n_trials=20)\n",
        "        print(f\"Лучшее значение для {ds}: {study.best_value:.4f}\")\n",
        "        print(f\"Лучшие гиперпараметры для {ds}: {study.best_params}\")\n",
        "        results[ds] = {\"best_value\": study.best_value, \"best_params\": study.best_params}\n",
        "\n",
        "    print(\"\\n=== Итоговые результаты MLP ===\")\n",
        "    for ds in dataset_names:\n",
        "        print(f\"{ds}: Acc = {results[ds]['best_value']:.4f}, Params = {results[ds]['best_params']}\")\n"
      ],
      "metadata": {
        "id": "kf0kiZ_153R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My GCN"
      ],
      "metadata": {
        "id": "Yoi8r9EA6wVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# GCN модель\n",
        "class GCNGraphClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        if x is None:\n",
        "            num_nodes = data.num_nodes\n",
        "            x = torch.ones((num_nodes, 1), device=edge_index.device)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Тренировка и тест\n",
        "def train_epoch(model, optimizer, criterion, loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# байес\n",
        "def objective_gcn(trial, dataset_name, device):\n",
        "    hidden_channels = trial.suggest_int(\"hidden_channels\", 32, 128, step=16)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-2)\n",
        "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    dataset = TUDataset(root='/tmp/' + dataset_name, name=dataset_name)\n",
        "    dataset = dataset.shuffle()\n",
        "    split_idx = int(0.8 * len(dataset))\n",
        "    train_dataset = dataset[:split_idx]\n",
        "    test_dataset = dataset[split_idx:]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    in_channels = dataset.num_node_features\n",
        "    if in_channels == 0:\n",
        "        in_channels = 1\n",
        "\n",
        "    model = GCNGraphClassifier(\n",
        "        in_channels=in_channels,\n",
        "        hidden_channels=hidden_channels,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=dataset.num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        train_epoch(model, optimizer, criterion, train_loader, device)\n",
        "        acc = test_epoch(model, test_loader, device)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        trial.report(acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return best_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset_names = [\"MUTAG\", \"PROTEINS\", \"IMDB-BINARY\", \"NCI1\"]\n",
        "    results = {}\n",
        "    for ds in dataset_names:\n",
        "        print(f\"\\n=== Оптимизация для датасета: {ds} (GCN) ===\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(lambda trial: objective_gcn(trial, ds, device), n_trials=20)\n",
        "        print(f\"Лучшее значение для {ds}: {study.best_value:.4f}\")\n",
        "        print(f\"Лучшие гиперпараметры для {ds}: {study.best_params}\")\n",
        "        results[ds] = {\"best_value\": study.best_value, \"best_params\": study.best_params}\n",
        "\n",
        "    print(\"\\n=== Итоговые результаты GCN ===\")\n",
        "    for ds in dataset_names:\n",
        "        print(f\"{ds}: Acc = {results[ds]['best_value']:.4f}, Params = {results[ds]['best_params']}\")\n"
      ],
      "metadata": {
        "id": "cmzUgqwi6xjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}